from dotenv import load_dotenv
load_dotenv()

import os
from flask import Flask, jsonify, request
import pandas as pd
import numpy as np
from flask_cors import CORS
from sklearn.ensemble import RandomForestClassifier
from supabase_client import get_supabase
from copilot_routes import copilot_bp, init_copilot
import os


app = Flask(__name__)
CORS(app)

# Initialize Supabase client
supabase = get_supabase()

# Load and clean the dataset
#df = pd.read_csv('train.csv')

def load_data_from_supabase():
    """Load Titanic data from Supabase instead of CSV"""
    try:
        print("üì• Loading data from Supabase...")
        
        # Fetch all data from Supabase
        response = supabase.table('Project_1').select('*').execute()
        
        if not response.data:
            raise Exception("No data found in Supabase")
        
        # Convert to DataFrame
        df = pd.DataFrame(response.data)
        print(f"‚úÖ Successfully loaded {len(df)} records from Supabase")
        
        return df
    
    except Exception as e:
        print(f"‚ùå Error loading data from Supabase: {e}")
        # Fallback to local CSV if Supabase fails
        print("üîÑ Falling back to local CSV...")
        return pd.read_csv('train.csv')

# Load data from Supabase
df = load_data_from_supabase()

def clean_titanic_data(df):
    """Clean the Titanic dataset with proper type handling for Supabase data"""
    clean_df = df.copy()
    
    print(f"üîß Data types before cleaning: {clean_df.dtypes}")
    
    # 1. Convert ALL data types that should be numeric (Supabase might have different types than CSV)
    numeric_columns = ['Age', 'Fare', 'SibSp', 'Parch', 'Pclass', 'Survived', 'PassengerId']
    
    for col in numeric_columns:
        if col in clean_df.columns:
            # Convert to string first, then to numeric to handle any data type issues
            clean_df[col] = pd.to_numeric(clean_df[col].astype(str), errors='coerce')
            print(f"   Converted {col} from {df[col].dtype} to {clean_df[col].dtype}")
    
    # 2. Handle missing values (without inplace to avoid warnings)
    if 'Age' in clean_df.columns:
        clean_df['Age'] = clean_df['Age'].fillna(clean_df['Age'].median())
    if 'Embarked' in clean_df.columns:
        clean_df['Embarked'] = clean_df['Embarked'].fillna(clean_df['Embarked'].mode()[0] if not clean_df['Embarked'].mode().empty else 'S')
    if 'Fare' in clean_df.columns:
        clean_df['Fare'] = clean_df['Fare'].fillna(clean_df['Fare'].median())
    
    # 3. Extract titles from Names
    if 'Name' in clean_df.columns:
        clean_df['Title'] = clean_df['Name'].str.extract(r' ([A-Za-z]+)\.', expand=False)
        
        title_mapping = {
            'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master', 
            'Dr': 'Officer', 'Rev': 'Officer', 'Col': 'Officer', 'Major': 'Officer',
            'Mlle': 'Miss', 'Ms': 'Miss', 'Lady': 'Royalty', 'Countess': 'Royalty',
            'Don': 'Royalty', 'Dona': 'Royalty', 'Mme': 'Mrs', 'Sir': 'Royalty', 
            'Jonkheer': 'Royalty', 'Capt': 'Officer'
        }
        clean_df['Title'] = clean_df['Title'].map(title_mapping)
        clean_df['Title'] = clean_df['Title'].fillna('Other')
    
    # 4. Family features (with proper type conversion)
    if 'SibSp' in clean_df.columns and 'Parch' in clean_df.columns:
        # Ensure numeric types - FIXED: Convert to string first, then to numeric
        clean_df['SibSp'] = pd.to_numeric(clean_df['SibSp'].astype(str), errors='coerce').fillna(0)
        clean_df['Parch'] = pd.to_numeric(clean_df['Parch'].astype(str), errors='coerce').fillna(0)
        
        clean_df['FamilySize'] = clean_df['SibSp'] + clean_df['Parch'] + 1
        clean_df['IsAlone'] = (clean_df['FamilySize'] == 1).astype(int)
    
    # 5. Drop columns that might cause issues
    columns_to_drop = ['Cabin', 'Ticket', 'Name', 'id', 'created_at']
    clean_df = clean_df.drop(columns=[col for col in columns_to_drop if col in clean_df.columns])
    
    print(f"‚úÖ Data cleaning complete. Final shape: {clean_df.shape}")
    print(f"üîß Data types after cleaning: {clean_df.dtypes}")
    return clean_df
# Clean the data
cleaned_df = clean_titanic_data(df)
# Initialize AI Copilot
init_copilot(cleaned_df)  # Add this line

# Register copilot blueprint
app.register_blueprint(copilot_bp)

def convert_to_serializable(obj):
    """Convert numpy/pandas types to JSON-serializable types"""
    if pd.isna(obj):
        return None
    elif isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        return float(obj)
    elif isinstance(obj, (bool, np.bool_)):
        return bool(obj)
    elif isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    elif isinstance(obj, (list, tuple)):
        return [convert_to_serializable(item) for item in obj]
    elif isinstance(obj, dict):
        return {key: convert_to_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    else:
        return obj

def save_prediction_to_supabase(passenger_data, prediction, probability, actual=None):
    """Save prediction results to Supabase"""
    try:
        data = {
            'passenger_data': passenger_data,
            'predicted_survival': prediction,
            'survival_probability': probability,
            'actual_survival': actual,
            'prediction_correct': actual is not None and prediction == actual,
            'model_type': 'Random Forest'
        }
        
        result = supabase.table('predictions').insert(data).execute()
        print(f"‚úÖ Prediction saved to Supabase: {prediction}")
        return result
    except Exception as e:
        print(f"‚ùå Error saving prediction to Supabase: {e}")
        return None

def save_model_run_to_supabase(accuracy, train_samples, test_samples, feature_count):
    """Save model run details to Supabase"""
    try:
        data = {
            'accuracy': accuracy,
            'training_samples': train_samples,
            'testing_samples': test_samples,
            'feature_count': feature_count,
            'model_type': 'Random Forest'
        }
        
        result = supabase.table('model_logs').insert(data).execute()
        print(f"‚úÖ Model run saved to Supabase: {accuracy:.3f} accuracy")
        return result
    except Exception as e:
        print(f"‚ùå Error saving model run to Supabase: {e}")
        return None

@app.route('/api/supabase/predictions', methods=['GET'])
def get_supabase_predictions():
    """Get all predictions from Supabase"""
    try:
        result = supabase.table('predictions').select('*').order('created_at', desc=True).execute()
        return jsonify({
            'predictions': result.data,
            'count': len(result.data)
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/supabase/model-runs', methods=['GET'])
def get_supabase_model_runs():
    """Get all model runs from Supabase"""
    try:
        result = supabase.table('model_logs').select('*').order('created_at', desc=True).execute()
        return jsonify({
            'model_runs': result.data,
            'count': len(result.data)
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/supabase/health', methods=['GET'])
def supabase_health_check():
    """Check Supabase connection and data"""
    try:
        # Use your actual table name 'Project_1' instead of 'titanic_passengers'
        response = supabase.table('Project_1').select('PassengerId', count='exact').limit(1).execute()
        return jsonify({
            'status': 'connected',
            'database': 'Supabase',
            'total_passengers': response.count,
            'message': 'Successfully connected to Supabase'
        })
    except Exception as e:
        return jsonify({
            'status': 'disconnected',
            'error': str(e)
        }), 500
     
# Basic endpoints
@app.route('/api/test', methods=['GET'])
def test():
    return jsonify({"message": "Flask API is working!", "data_shape": list(cleaned_df.shape)})

@app.route('/api/head', methods=['GET'])
def head():
    """Get first 20 rows of CLEANED data"""
    data = cleaned_df.head(20).to_dict('records')
    serializable_data = [convert_to_serializable(record) for record in data]
    return jsonify(serializable_data)

@app.route('/api/info', methods=['GET'])
def info():
    """Get info about CLEANED dataset"""
    info = {
        "columns": list(cleaned_df.columns),
        "shape": list(cleaned_df.shape),
        "missing_values": convert_to_serializable(cleaned_df.isnull().sum().to_dict()),
        "data_types": cleaned_df.dtypes.astype(str).to_dict()
    }
    return jsonify(info)

# EDA endpoints
@app.route('/api/summary', methods=['GET'])
def summary():
    """Get statistical summary of numerical columns"""
    summary_dict = cleaned_df.describe().to_dict()
    serializable_summary = convert_to_serializable(summary_dict)
    return jsonify(serializable_summary)

@app.route('/api/survival_rates', methods=['GET'])
def survival_rates():
    """Get survival rates by different categories"""
    survival_by_class = cleaned_df.groupby('Pclass')['Survived'].mean().to_dict()
    survival_by_sex = cleaned_df.groupby('Sex')['Survived'].mean().to_dict()
    survival_by_embarked = cleaned_df.groupby('Embarked')['Survived'].mean().to_dict()
    survival_by_title = cleaned_df.groupby('Title')['Survived'].mean().to_dict()
    
    return jsonify({
        'by_class': convert_to_serializable(survival_by_class),
        'by_sex': convert_to_serializable(survival_by_sex),
        'by_embarked': convert_to_serializable(survival_by_embarked),
        'by_title': convert_to_serializable(survival_by_title)
    })

@app.route('/api/correlation', methods=['GET'])
def correlation():
    """Get correlation matrix for numerical features"""
    numerical_df = cleaned_df.select_dtypes(include=[np.number])
    corr_dict = numerical_df.corr().to_dict()
    serializable_corr = convert_to_serializable(corr_dict)
    return jsonify(serializable_corr)

# Data endpoints with pagination
@app.route('/api/data', methods=['GET'])
def get_all_data():
    """Get all cleaned data with optional pagination"""
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 10, type=int)
    
    start_idx = (page - 1) * per_page
    end_idx = start_idx + per_page
    
    paginated_data = cleaned_df.iloc[start_idx:end_idx]
    
    data_records = []
    for record in paginated_data.to_dict('records'):
        serializable_record = convert_to_serializable(record)
        data_records.append(serializable_record)
    
    return jsonify({
        'data': data_records,
        'total_records': len(cleaned_df),
        'page': page,
        'per_page': per_page,
        'total_pages': (len(cleaned_df) + per_page - 1) // per_page
    })

@app.route('/api/data/count', methods=['GET'])
def data_count():
    """Get total record count"""
    return jsonify({'total_records': len(cleaned_df)})

@app.route('/api/regression/survival', methods=['GET'])
def survival_regression():
    """Perform Logistic Regression with ENGINEERED FEATURES"""
    try:
        from sklearn.linear_model import LogisticRegression
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import accuracy_score
        from sklearn.preprocessing import LabelEncoder
        from sklearn.preprocessing import StandardScaler
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.model_selection import cross_val_score, StratifiedKFold
        print("üöÄ RUNNING REAL MODEL WITH ENGINEERED FEATURES")

        # DEBUG: Check data types before starting
        print(f"üîç DEBUG: cleaned_df dtypes: {cleaned_df.dtypes}")
        print(f"üîç DEBUG: Survived column type: {cleaned_df['Survived'].dtype}")
        print(f"üîç DEBUG: Sample Survived values: {cleaned_df['Survived'].head()}")        
        # Prepare features and target
        features_df = cleaned_df.copy()

        # FORCE CONVERT CRITICAL COLUMNS TO NUMERIC (double safety)
        critical_numeric_cols = ['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']
        for col in critical_numeric_cols:
            if col in features_df.columns:
                features_df[col] = pd.to_numeric(features_df[col], errors='coerce')
        
        # Remove any rows with NaN in Survived (target variable)
        features_df = features_df.dropna(subset=['Survived'])

        # Encode categorical variables
        le_sex = LabelEncoder()
        le_embarked = LabelEncoder()
        le_title = LabelEncoder()

        features_df['FamilySize'] = features_df['SibSp'] + features_df['Parch'] + 1
        features_df['IsAlone'] = (features_df['FamilySize'] == 1).astype(int)
        features_df['IsChild'] = (features_df['Age'] < 12).astype(int)
        features_df['IsFemale'] = (features_df['Sex'] == 'female').astype(int)
        features_df['IsRich'] = ((features_df['Pclass'] == 1) & (features_df['Fare'] > 50)).astype(int)
        features_df['Sex_encoded'] = le_sex.fit_transform(features_df['Sex'])
        features_df['Embarked_encoded'] = le_embarked.fit_transform(features_df['Embarked'].fillna('S'))
        features_df['Title_encoded'] = le_title.fit_transform(features_df['Title'])

        # Simple but effective feature set
        feature_columns = [
            'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',
            'Sex_encoded', 'Embarked_encoded', 'Title_encoded',
            'FamilySize', 'IsAlone', 'IsChild', 'IsFemale', 'IsRich' # Additional engineered features
        ]
        scaler = StandardScaler()


        X = features_df[feature_columns]
        y = features_df['Survived']
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # Train model
        #model = LogisticRegression(random_state=42, max_iter=1000)
        model = RandomForestClassifier(
            n_estimators=200,       # More trees
            max_depth=10,           # Deeper trees
            min_samples_split=3,    # More flexible splitting
            min_samples_leaf=1,     # More flexible leaves
            max_features='sqrt',    # Better feature selection
            random_state=42
        )    
        # # Option B: Balanced approach  
        # model = RandomForestClassifier(
        #     n_estimators=200,
        #     max_depth=8,
        #     min_samples_split=5,
        #     min_samples_leaf=2,
        #     max_features=0.7,
        #     random_state=42
        # )
        # CROSS-VALIDATION
        print("üìä RUNNING 5-FOLD CROSS-VALIDATION...")
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
        
        cv_mean = cv_scores.mean()
        cv_std = cv_scores.std()
        
        print(f"üéØ CROSS-VALIDATION RESULTS:")
        print(f"   Fold Scores: {[f'{score:.3f}' for score in cv_scores]}")
        print(f"   Mean CV Accuracy: {cv_mean:.3f} (+/- {cv_std * 2:.3f})")
        print(f"   Range: {cv_scores.min():.3f} - {cv_scores.max():.3f}")
        model.fit(X_train, y_train)
        def predict_survival_probability(passenger_features):
            scaled_features = scaler.transform([passenger_features])
            return model.predict_proba(scaled_features)[0][1]
        # Make predictions
        # y_pred = model.predict_survival_probability(X_test)
        y_pred = model.predict(X_test)
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        print(f"üéØ REAL MODEL ACCURACY: {accuracy:.3f} ({accuracy*100:.1f}%)")
        
        # Calculate accuracies
        # test_accuracy = accuracy_score(y_test, y_pred)
        train_accuracy = accuracy_score(y_train, model.predict(X_train))
        overfitting_gap = train_accuracy - accuracy
        
        print(f"üéØ FINAL RESULTS:")
        print(f"   Test Accuracy: {accuracy:.3f}")
        print(f"   Train Accuracy: {train_accuracy:.3f}")
        print(f"   Overfitting Gap: {overfitting_gap:.3f} (ideal: < 0.05)")
        # Feature importance - CONVERT TO NATIVE PYTHON TYPES
        feature_importance = {}
        # for col, coef in zip(feature_columns, model.coef_[0]):
        #     feature_importance[col] = float(coef)  # Convert numpy to native float
        for col, importance in zip(feature_columns, model.feature_importances_):
            feature_importance[col] = float(importance)        
        # Sample predictions with PROPER SERIALIZATION
        sample_indices = X_test.index[:6]
        sample_predictions = []
        
        for idx in sample_indices:
            # Convert passenger data to serializable format
            passenger_data = {}
            for col, value in cleaned_df.loc[idx].items():
                passenger_data[col] = convert_to_serializable(value)
            
            # Get prediction
            prediction = int(model.predict([X.loc[idx]])[0])
            probability = float(model.predict_proba([X.loc[idx]])[0][1])  # Convert to native float
            
            sample_predictions.append({
                'passenger_data': passenger_data,
                'predicted_survival': prediction,
                'actual_survival': int(y.loc[idx]),
                'survival_probability': round(probability, 3),
                'correct': bool(prediction == y.loc[idx])  # Convert numpy bool to Python bool
            })

        # # Save model run to Supabase
        # model_run = save_model_run_to_supabase(
        #     accuracy=float(accuracy),
        #     train_samples=int(len(X_train)),
        #     test_samples=int(len(X_test)),
        #     feature_count=int(len(feature_columns))
        # )
        
        # # Save sample predictions to Supabase
        # for pred in sample_predictions:
        #     save_prediction_to_supabase(
        #         passenger_data=pred['passenger_data'],
        #         prediction=pred['predicted_survival'],
        #         probability=pred['survival_probability'],
        #         actual=pred['actual_survival']
        #     )
                
        # Prepare final response with ALL NATIVE TYPES
        response_data = {
            'model_performance': {
                'accuracy': float(accuracy),  # Convert to native float
                'training_samples': int(len(X_train)),
                'testing_samples': int(len(X_test)),
                'model_type': 'Random Forest Classifier',
                'feature_count': int(len(feature_columns))
            },
            'feature_importance': feature_importance,
            'sample_predictions': sample_predictions,
            'coefficients': feature_importance,
            'status': 'real_model_success'
        }
        
        print("‚úÖ SUCCESS: Real model completed without errors")
        return jsonify(response_data)
        
    except Exception as e:
        print(f"‚ùå REAL MODEL FAILED: {e}")
        import traceback
        print(f"üîç FULL ERROR TRACEBACK: {traceback.format_exc()}")
        
        # Return proper error instead of falling back to mock
        return jsonify({
            'error': f'Model training failed: {str(e)}',
            'status': 'error'
        }), 500

@app.route('/api/regression/feature_analysis', methods=['GET'])
def feature_analysis():
    """Analyze feature relationships with survival - PROPERLY GROUPED"""
    try:
        analysis = {}
        
        # Define how to group each feature meaningfully
        features_to_analyze = {
            'Pclass': 'categorical',      # Already categorical (1, 2, 3)
            'Sex': 'categorical',         # Already categorical (male, female)
            'Age': 'binned',              # Needs binning (0-10, 10-20, etc.)
            'SibSp': 'discrete',          # Limited values (0, 1, 2, 3+)
            'Parch': 'discrete',          # Limited values (0, 1, 2, 3+)
            'Fare': 'binned',             # Needs binning (0-10, 10-50, etc.)
            'Embarked': 'categorical',    # Already categorical (C, Q, S)
            'Title': 'categorical',       # Already categorical
            'FamilySize': 'discrete'      # Limited values (1, 2, 3, 4+)
        }
        
        for feature, feature_type in features_to_analyze.items():
            try:
                if feature_type == 'categorical':
                    # Use actual categories
                    survival_data = cleaned_df.groupby(feature)['Survived'].mean().to_dict()
                    
                    analysis[feature] = {
                        'survival_by_group': convert_to_serializable(survival_data),
                        'correlation_with_survival': 'N/A',
                        'mean_survival': 'N/A',
                        'feature_type': 'categorical'
                    }
                    
                elif feature_type == 'discrete':
                    # Use actual values (these have limited unique values)
                    survival_data = cleaned_df.groupby(feature)['Survived'].mean().to_dict()
                    
                    analysis[feature] = {
                        'survival_by_group': convert_to_serializable(survival_data),
                        'correlation_with_survival': float(cleaned_df[feature].corr(cleaned_df['Survived'])),
                        'mean_survival': {
                            'survived': float(cleaned_df[cleaned_df['Survived'] == 1][feature].mean()),
                            'died': float(cleaned_df[cleaned_df['Survived'] == 0][feature].mean())
                        },
                        'feature_type': 'discrete'
                    }
                    
                elif feature_type == 'binned':
                    # Create meaningful bins for continuous features
                    if feature == 'Age':
                        # Age groups: Children, Young Adults, Adults, Seniors
                        bins = [0, 12, 18, 35, 60, 100]
                        labels = ['Child (0-12)', 'Teen (13-18)', 'Young Adult (19-35)', 'Adult (36-60)', 'Senior (60+)']
                        cleaned_df['Age_group'] = pd.cut(cleaned_df['Age'], bins=bins, labels=labels)
                        survival_data = cleaned_df.groupby('Age_group')['Survived'].mean().to_dict()
                        
                    elif feature == 'Fare':
                        # Fare groups: Low, Medium, High, Luxury
                        bins = [0, 10, 30, 100, 600]
                        labels = ['Low (0-10)', 'Medium (10-30)', 'High (30-100)', 'Luxury (100+)']
                        cleaned_df['Fare_group'] = pd.cut(cleaned_df['Fare'], bins=bins, labels=labels)
                        survival_data = cleaned_df.groupby('Fare_group')['Survived'].mean().to_dict()
                    
                    analysis[feature] = {
                        'survival_by_group': convert_to_serializable(survival_data),
                        'correlation_with_survival': float(cleaned_df[feature].corr(cleaned_df['Survived'])),
                        'mean_survival': {
                            'survived': float(cleaned_df[cleaned_df['Survived'] == 1][feature].mean()),
                            'died': float(cleaned_df[cleaned_df['Survived'] == 0][feature].mean())
                        },
                        'feature_type': 'continuous'
                    }
                    
            except Exception as e:
                print(f"Error with feature {feature}: {e}")
                # Fallback: use simple grouping
                survival_data = cleaned_df.groupby(feature)['Survived'].mean().to_dict()
                analysis[feature] = {
                    'survival_by_group': convert_to_serializable(survival_data),
                    'correlation_with_survival': 'Error',
                    'mean_survival': 'Error',
                    'feature_type': 'error'
                }
        
        return jsonify(analysis)
        
    except Exception as e:
        print(f"Feature analysis error: {e}")
        return jsonify({'error': f'Feature analysis failed: {str(e)}'}), 500

@app.route('/api/regression/predict', methods=['GET', 'POST'])  # Allow both GET and POST
def predict_survival():
    """Predict survival for custom input (mock version)"""
    try:
        if request.method == 'GET':
            # Return instructions for POST request
            return jsonify({
                'message': 'Send a POST request with passenger data',
                'example_post_data': {
                    'Sex': 'female',
                    'Pclass': 1,
                    'Age': 25,
                    'Fare': 100
                }
            })
        
        # Handle POST request
        data = request.json
        
        # Mock prediction based on input rules
        survival_prob = 0.5
        
        if data.get('Sex') == 'female':
            survival_prob += 0.3
        if data.get('Pclass') == 1:
            survival_prob += 0.2
        if data.get('Age', 30) < 16:
            survival_prob += 0.1
        if data.get('Fare', 0) > 50:
            survival_prob += 0.1
            
        survival_prob = max(0.1, min(0.9, survival_prob))
        prediction = 1 if survival_prob > 0.5 else 0
        
        return jsonify({
            'prediction': prediction,
            'survival_probability': round(survival_prob, 3),
            'confidence': 'high' if abs(survival_prob - 0.5) > 0.3 else 'medium',
            'factors_considered': ['Sex', 'Pclass', 'Age', 'Fare'],
            'note': 'This is a rule-based mock prediction'
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# Health check endpoint
@app.route('/api/health', methods=['GET'])
def health_check():
    return jsonify({
        'status': 'healthy',
        'dataset_size': list(cleaned_df.shape),
        'columns': list(cleaned_df.columns),
        'missing_values': convert_to_serializable(cleaned_df.isnull().sum().to_dict())
    })

if __name__ == '__main__':
    # app.run(debug=True, port=5000)
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port)